{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "from pandas import *\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "from helper760 import read_inputs\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "n_split=8           # define number for k\n",
    "re_train_1 = True  # define for the step 1 (CT+Clinic -> Death Day), do we want to retrain the model?\n",
    "re_train_2 = False  # define for the step 2 (CT -> Death Day), do we want to retrain the model?\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Use both clinic and ct data to predict death day\n",
    "Note that we have only around 549 real samples with known death day for us to train the model\n",
    "\n",
    "Currently, we use these 549 samples as training set, and the rest will be the prediction set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data for step 1\n",
    "Clininc_Data,Outcome_Data,CT_Data = read_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out who has known death day\n",
    "n_sample = len(Outcome_Data[0])\n",
    "fill_train_idx = []\n",
    "fill_test_idx = []\n",
    "\n",
    "for i in range(n_sample):\n",
    "    if Outcome_Data[0][i]!=0:\n",
    "        fill_train_idx.append(i)\n",
    "    else:\n",
    "        fill_test_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9223, 11)\n",
      "(9223, 10)\n",
      "(9223, 21)\n",
      "(549, 21)\n",
      "(8674, 21)\n",
      "(549, 1)\n"
     ]
    }
   ],
   "source": [
    "# prepare the data for training\n",
    "# in this case we use both the clinic data and ct data\n",
    "X1 = np.array(CT_Data)\n",
    "X1 = X1.T\n",
    "X1 = X1.astype(float)\n",
    "print(X1.shape)\n",
    "\n",
    "X2 = np.array(Clininc_Data)\n",
    "X2 = X2.T\n",
    "X2 = X2[:,4:14]\n",
    "X2 = X2.astype(float)\n",
    "print(X2.shape)\n",
    "\n",
    "X = np.hstack((X1,X2))\n",
    "print(X.shape)\n",
    "\n",
    "X_fill_train = X[fill_train_idx,:]\n",
    "print(X_fill_train.shape)\n",
    "\n",
    "X_fill_test = X[fill_test_idx,:]\n",
    "print(X_fill_test.shape)\n",
    "\n",
    "y_fill_train = np.array(Outcome_Data[0])\n",
    "y_fill_train = y_fill_train.astype(int)\n",
    "y_fill_train = np.asmatrix(y_fill_train)\n",
    "y_fill_train = y_fill_train.T\n",
    "y_fill_train = y_fill_train[fill_train_idx,:]\n",
    "print(y_fill_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fold 0 total 8 folds\n",
      "Epoch 1/3000\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3143507.7500 - mean_squared_error: 3143507.7500\n",
      "Epoch 2/3000\n",
      "96/96 [==============================] - 0s 883us/step - loss: 2392999.5000 - mean_squared_error: 2392999.5000\n",
      "Epoch 3/3000\n",
      "96/96 [==============================] - 0s 884us/step - loss: 2299789.7500 - mean_squared_error: 2299789.7500\n",
      "Epoch 4/3000\n",
      "96/96 [==============================] - 0s 871us/step - loss: 2273717.2500 - mean_squared_error: 2273717.2500\n",
      "Epoch 5/3000\n",
      "96/96 [==============================] - 0s 854us/step - loss: 2046908.6250 - mean_squared_error: 2046908.6250\n",
      "Epoch 6/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 2047250.2500 - mean_squared_error: 2047250.2500\n",
      "Epoch 7/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1968808.8750 - mean_squared_error: 1968808.8750\n",
      "Epoch 8/3000\n",
      "96/96 [==============================] - 0s 836us/step - loss: 1912102.0000 - mean_squared_error: 1912102.0000\n",
      "Epoch 9/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1894886.1250 - mean_squared_error: 1894886.1250\n",
      "Epoch 10/3000\n",
      "96/96 [==============================] - 0s 842us/step - loss: 1878100.5000 - mean_squared_error: 1878100.5000\n",
      "Epoch 11/3000\n",
      "96/96 [==============================] - 0s 883us/step - loss: 1894500.0000 - mean_squared_error: 1894500.0000\n",
      "Epoch 12/3000\n",
      "96/96 [==============================] - 0s 903us/step - loss: 1882434.7500 - mean_squared_error: 1882434.7500\n",
      "Epoch 13/3000\n",
      "96/96 [==============================] - 0s 902us/step - loss: 1913328.1250 - mean_squared_error: 1913328.1250\n",
      "Epoch 14/3000\n",
      "96/96 [==============================] - 0s 891us/step - loss: 1887999.3750 - mean_squared_error: 1887999.3750\n",
      "Epoch 15/3000\n",
      "96/96 [==============================] - 0s 895us/step - loss: 1865485.2500 - mean_squared_error: 1865485.2500\n",
      "Epoch 16/3000\n",
      "96/96 [==============================] - 0s 900us/step - loss: 1823719.5000 - mean_squared_error: 1823719.5000\n",
      "Epoch 17/3000\n",
      "96/96 [==============================] - 0s 887us/step - loss: 1875855.5000 - mean_squared_error: 1875855.5000\n",
      "Epoch 18/3000\n",
      "96/96 [==============================] - 0s 894us/step - loss: 1798878.0000 - mean_squared_error: 1798878.1250\n",
      "Epoch 19/3000\n",
      "96/96 [==============================] - 0s 992us/step - loss: 1812619.3750 - mean_squared_error: 1812619.3750\n",
      "Epoch 20/3000\n",
      "96/96 [==============================] - 0s 885us/step - loss: 1784409.2500 - mean_squared_error: 1784409.2500\n",
      "Epoch 21/3000\n",
      "96/96 [==============================] - 0s 902us/step - loss: 1798365.3750 - mean_squared_error: 1798365.3750\n",
      "Epoch 22/3000\n",
      "96/96 [==============================] - 0s 900us/step - loss: 1761303.8750 - mean_squared_error: 1761303.8750\n",
      "Epoch 23/3000\n",
      "96/96 [==============================] - 0s 897us/step - loss: 1826114.0000 - mean_squared_error: 1826114.0000\n",
      "Epoch 24/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 1766203.8750 - mean_squared_error: 1766203.8750\n",
      "Epoch 25/3000\n",
      "96/96 [==============================] - 0s 872us/step - loss: 1802748.1250 - mean_squared_error: 1802748.1250\n",
      "Epoch 26/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 1813288.8750 - mean_squared_error: 1813288.8750\n",
      "Epoch 27/3000\n",
      "96/96 [==============================] - 0s 834us/step - loss: 1822031.2500 - mean_squared_error: 1822031.1250\n",
      "Epoch 28/3000\n",
      "96/96 [==============================] - 0s 825us/step - loss: 1743915.2500 - mean_squared_error: 1743915.3750\n",
      "Epoch 29/3000\n",
      "96/96 [==============================] - 0s 832us/step - loss: 1695744.3750 - mean_squared_error: 1695744.3750\n",
      "Epoch 30/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 1831560.8750 - mean_squared_error: 1831560.8750\n",
      "Epoch 31/3000\n",
      "96/96 [==============================] - 0s 878us/step - loss: 1746200.1250 - mean_squared_error: 1746200.1250\n",
      "Epoch 32/3000\n",
      "96/96 [==============================] - 0s 867us/step - loss: 1761871.1250 - mean_squared_error: 1761871.1250\n",
      "Epoch 33/3000\n",
      "96/96 [==============================] - 0s 863us/step - loss: 1694344.6250 - mean_squared_error: 1694344.6250\n",
      "Epoch 34/3000\n",
      "96/96 [==============================] - 0s 850us/step - loss: 1763584.8750 - mean_squared_error: 1763584.8750\n",
      "Epoch 35/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1815898.7500 - mean_squared_error: 1815898.7500\n",
      "Epoch 36/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1733020.8750 - mean_squared_error: 1733020.8750\n",
      "Epoch 37/3000\n",
      "96/96 [==============================] - 0s 836us/step - loss: 1717330.2500 - mean_squared_error: 1717330.2500\n",
      "Epoch 38/3000\n",
      "96/96 [==============================] - 0s 836us/step - loss: 1732212.1250 - mean_squared_error: 1732212.1250\n",
      "Epoch 39/3000\n",
      "96/96 [==============================] - 0s 855us/step - loss: 1805552.7500 - mean_squared_error: 1805552.7500\n",
      "Epoch 40/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1687188.2500 - mean_squared_error: 1687188.2500\n",
      "Epoch 41/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1732538.1250 - mean_squared_error: 1732538.1250\n",
      "Epoch 42/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1701320.3750 - mean_squared_error: 1701320.2500\n",
      "Epoch 43/3000\n",
      "96/96 [==============================] - 0s 832us/step - loss: 1706145.2500 - mean_squared_error: 1706145.2500\n",
      "Epoch 44/3000\n",
      "96/96 [==============================] - 0s 834us/step - loss: 1681984.0000 - mean_squared_error: 1681984.0000\n",
      "Epoch 45/3000\n",
      "96/96 [==============================] - 0s 831us/step - loss: 1714344.2500 - mean_squared_error: 1714344.2500\n",
      "Epoch 46/3000\n",
      "96/96 [==============================] - 0s 833us/step - loss: 1704870.1250 - mean_squared_error: 1704870.1250\n",
      "Epoch 47/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1709000.5000 - mean_squared_error: 1709000.6250\n",
      "Epoch 48/3000\n",
      "96/96 [==============================] - 0s 861us/step - loss: 1679348.6250 - mean_squared_error: 1679348.6250\n",
      "Epoch 49/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1716402.0000 - mean_squared_error: 1716402.0000\n",
      "Epoch 50/3000\n",
      "96/96 [==============================] - 0s 832us/step - loss: 1638361.1250 - mean_squared_error: 1638360.8750\n",
      "Epoch 51/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1653543.2500 - mean_squared_error: 1653543.2500\n",
      "Epoch 52/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1624417.7500 - mean_squared_error: 1624417.7500\n",
      "Epoch 53/3000\n",
      "96/96 [==============================] - 0s 829us/step - loss: 1678327.5000 - mean_squared_error: 1678327.5000\n",
      "Epoch 54/3000\n",
      "96/96 [==============================] - 0s 832us/step - loss: 1670766.1250 - mean_squared_error: 1670766.1250\n",
      "Epoch 55/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1700361.5000 - mean_squared_error: 1700361.5000\n",
      "Epoch 56/3000\n",
      "96/96 [==============================] - 0s 862us/step - loss: 1627849.1250 - mean_squared_error: 1627849.1250\n",
      "Epoch 57/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 1672546.1250 - mean_squared_error: 1672546.1250\n",
      "Epoch 58/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 1684744.0000 - mean_squared_error: 1684744.0000\n",
      "Epoch 59/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1606862.3750 - mean_squared_error: 1606862.3750\n",
      "Epoch 60/3000\n",
      "96/96 [==============================] - 0s 835us/step - loss: 1639342.0000 - mean_squared_error: 1639342.0000\n",
      "Epoch 61/3000\n",
      "96/96 [==============================] - 0s 834us/step - loss: 1702889.5000 - mean_squared_error: 1702889.5000\n",
      "Epoch 62/3000\n",
      "96/96 [==============================] - 0s 831us/step - loss: 1608305.7500 - mean_squared_error: 1608305.7500\n",
      "Epoch 63/3000\n",
      "96/96 [==============================] - 0s 831us/step - loss: 1591795.2500 - mean_squared_error: 1591795.2500\n",
      "Epoch 64/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1680156.7500 - mean_squared_error: 1680156.7500\n",
      "Epoch 65/3000\n",
      "96/96 [==============================] - 0s 866us/step - loss: 1599717.2500 - mean_squared_error: 1599717.1250\n",
      "Epoch 66/3000\n",
      "96/96 [==============================] - 0s 871us/step - loss: 1607049.2500 - mean_squared_error: 1607049.2500\n",
      "Epoch 67/3000\n",
      "96/96 [==============================] - 0s 892us/step - loss: 1609884.2500 - mean_squared_error: 1609884.3750\n",
      "Epoch 68/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 1598684.5000 - mean_squared_error: 1598684.5000\n",
      "Epoch 69/3000\n",
      "96/96 [==============================] - 0s 855us/step - loss: 1652608.7500 - mean_squared_error: 1652608.7500\n",
      "Epoch 70/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 1582985.3750 - mean_squared_error: 1582985.3750\n",
      "Epoch 71/3000\n",
      "96/96 [==============================] - 0s 864us/step - loss: 1633675.1250 - mean_squared_error: 1633675.1250\n",
      "Epoch 72/3000\n",
      "96/96 [==============================] - 0s 861us/step - loss: 1641290.3750 - mean_squared_error: 1641290.3750\n",
      "Epoch 73/3000\n",
      "96/96 [==============================] - 0s 879us/step - loss: 1598868.6250 - mean_squared_error: 1598868.6250\n",
      "Epoch 74/3000\n",
      "96/96 [==============================] - 0s 855us/step - loss: 1628558.7500 - mean_squared_error: 1628558.7500\n",
      "Epoch 75/3000\n",
      "96/96 [==============================] - 0s 873us/step - loss: 1609231.5000 - mean_squared_error: 1609231.5000\n",
      "Epoch 76/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 1680468.7500 - mean_squared_error: 1680468.7500\n",
      "Epoch 77/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 1620060.0000 - mean_squared_error: 1620060.0000\n",
      "Epoch 78/3000\n",
      "96/96 [==============================] - 0s 862us/step - loss: 1649008.1250 - mean_squared_error: 1649008.2500\n",
      "Epoch 79/3000\n",
      "96/96 [==============================] - 0s 842us/step - loss: 1572398.7500 - mean_squared_error: 1572398.7500\n",
      "Epoch 80/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1619189.2500 - mean_squared_error: 1619189.2500\n",
      "Epoch 81/3000\n",
      "96/96 [==============================] - 0s 850us/step - loss: 1533714.0000 - mean_squared_error: 1533714.0000\n",
      "Epoch 82/3000\n",
      "96/96 [==============================] - 0s 835us/step - loss: 1556769.6250 - mean_squared_error: 1556769.5000\n",
      "Epoch 83/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 1549740.1250 - mean_squared_error: 1549740.1250\n",
      "Epoch 84/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 1566203.7500 - mean_squared_error: 1566203.7500\n",
      "Epoch 85/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1560325.8750 - mean_squared_error: 1560325.8750\n",
      "Epoch 86/3000\n",
      "96/96 [==============================] - 0s 872us/step - loss: 1585775.8750 - mean_squared_error: 1585775.8750\n",
      "Epoch 87/3000\n",
      "96/96 [==============================] - 0s 870us/step - loss: 1534860.0000 - mean_squared_error: 1534860.0000\n",
      "Epoch 88/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 1676578.7500 - mean_squared_error: 1676578.7500\n",
      "Epoch 89/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 1602794.7500 - mean_squared_error: 1602794.7500\n",
      "Epoch 90/3000\n",
      "96/96 [==============================] - 0s 861us/step - loss: 1583946.2500 - mean_squared_error: 1583946.2500\n",
      "Epoch 91/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 1583437.7500 - mean_squared_error: 1583437.6250\n",
      "Epoch 92/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1537074.6250 - mean_squared_error: 1537074.6250\n",
      "Epoch 93/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1617831.3750 - mean_squared_error: 1617831.2500\n",
      "Epoch 94/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1551054.2500 - mean_squared_error: 1551054.2500\n",
      "Epoch 95/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 1570709.6250 - mean_squared_error: 1570709.6250\n",
      "Epoch 96/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 1508385.5000 - mean_squared_error: 1508385.5000\n",
      "Epoch 97/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1509983.5000 - mean_squared_error: 1509983.5000\n",
      "Epoch 98/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 1549332.1250 - mean_squared_error: 1549332.1250\n",
      "Epoch 99/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1546273.2500 - mean_squared_error: 1546273.2500\n",
      "Epoch 100/3000\n",
      "96/96 [==============================] - 0s 867us/step - loss: 1517193.8750 - mean_squared_error: 1517193.8750\n",
      "Epoch 101/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 1510848.3750 - mean_squared_error: 1510848.5000\n",
      "Epoch 102/3000\n",
      "96/96 [==============================] - 0s 866us/step - loss: 1502182.2500 - mean_squared_error: 1502182.2500\n",
      "Epoch 103/3000\n",
      "96/96 [==============================] - 0s 871us/step - loss: 1493085.1250 - mean_squared_error: 1493085.1250\n",
      "Epoch 104/3000\n",
      "96/96 [==============================] - 0s 855us/step - loss: 1478856.3750 - mean_squared_error: 1478856.2500\n",
      "Epoch 105/3000\n",
      "96/96 [==============================] - 0s 863us/step - loss: 1520896.7500 - mean_squared_error: 1520896.7500\n",
      "Epoch 106/3000\n",
      "96/96 [==============================] - 0s 872us/step - loss: 1490508.8750 - mean_squared_error: 1490508.8750\n",
      "Epoch 107/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1465469.2500 - mean_squared_error: 1465469.2500\n",
      "Epoch 108/3000\n",
      "96/96 [==============================] - 0s 832us/step - loss: 1551638.7500 - mean_squared_error: 1551638.7500\n",
      "Epoch 109/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1484140.0000 - mean_squared_error: 1484140.0000\n",
      "Epoch 110/3000\n",
      "96/96 [==============================] - 0s 859us/step - loss: 1549592.0000 - mean_squared_error: 1549592.0000\n",
      "Epoch 111/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 1479939.2500 - mean_squared_error: 1479939.2500\n",
      "Epoch 112/3000\n",
      "96/96 [==============================] - 0s 855us/step - loss: 1546600.3750 - mean_squared_error: 1546600.3750\n",
      "Epoch 113/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 1486174.1250 - mean_squared_error: 1486174.1250\n",
      "Epoch 114/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1475938.6250 - mean_squared_error: 1475938.6250\n",
      "Epoch 115/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 1443206.6250 - mean_squared_error: 1443206.6250\n",
      "Epoch 116/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 1451715.8750 - mean_squared_error: 1451715.8750\n",
      "Epoch 117/3000\n",
      "96/96 [==============================] - 0s 863us/step - loss: 1509665.8750 - mean_squared_error: 1509665.8750\n",
      "Epoch 118/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1469461.5000 - mean_squared_error: 1469461.5000\n",
      "Epoch 119/3000\n",
      "96/96 [==============================] - 0s 850us/step - loss: 1555488.7500 - mean_squared_error: 1555488.7500\n",
      "Epoch 120/3000\n",
      "96/96 [==============================] - 0s 859us/step - loss: 1416587.3750 - mean_squared_error: 1416587.3750\n",
      "Epoch 121/3000\n",
      "96/96 [==============================] - 0s 847us/step - loss: 1575396.7500 - mean_squared_error: 1575396.7500\n",
      "Epoch 122/3000\n",
      "96/96 [==============================] - 0s 847us/step - loss: 1433372.8750 - mean_squared_error: 1433372.8750\n",
      "Epoch 123/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 1461569.2500 - mean_squared_error: 1461569.2500\n",
      "Epoch 124/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1414914.2500 - mean_squared_error: 1414914.2500\n",
      "Epoch 125/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 1440839.5000 - mean_squared_error: 1440839.3750\n",
      "Epoch 126/3000\n",
      "96/96 [==============================] - 0s 880us/step - loss: 1546614.8750 - mean_squared_error: 1546614.8750\n",
      "Epoch 127/3000\n",
      "96/96 [==============================] - 0s 889us/step - loss: 1410172.7500 - mean_squared_error: 1410172.7500\n",
      "Epoch 128/3000\n",
      "96/96 [==============================] - 0s 891us/step - loss: 1397874.2500 - mean_squared_error: 1397874.2500\n",
      "Epoch 129/3000\n",
      "96/96 [==============================] - 0s 863us/step - loss: 1362214.6250 - mean_squared_error: 1362214.6250\n",
      "Epoch 130/3000\n",
      "96/96 [==============================] - 0s 850us/step - loss: 1419592.0000 - mean_squared_error: 1419592.0000\n",
      "Epoch 131/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 1413813.6250 - mean_squared_error: 1413813.6250\n",
      "Epoch 132/3000\n",
      "96/96 [==============================] - 0s 849us/step - loss: 1391001.3750 - mean_squared_error: 1391001.2500\n",
      "Epoch 133/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1426678.5000 - mean_squared_error: 1426678.5000\n",
      "Epoch 134/3000\n",
      "96/96 [==============================] - 0s 835us/step - loss: 1373940.8750 - mean_squared_error: 1373940.8750\n",
      "Epoch 135/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1386319.1250 - mean_squared_error: 1386319.1250\n",
      "Epoch 136/3000\n",
      "96/96 [==============================] - 0s 834us/step - loss: 1371620.1250 - mean_squared_error: 1371620.1250\n",
      "Epoch 137/3000\n",
      "96/96 [==============================] - 0s 842us/step - loss: 1359597.8750 - mean_squared_error: 1359597.8750\n",
      "Epoch 138/3000\n",
      "96/96 [==============================] - 0s 836us/step - loss: 1380667.3750 - mean_squared_error: 1380667.3750\n",
      "Epoch 139/3000\n",
      "96/96 [==============================] - 0s 831us/step - loss: 1396260.2500 - mean_squared_error: 1396260.2500\n",
      "Epoch 140/3000\n",
      "96/96 [==============================] - 0s 835us/step - loss: 1379102.3750 - mean_squared_error: 1379102.3750\n",
      "Epoch 141/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1362923.7500 - mean_squared_error: 1362923.7500\n",
      "Epoch 142/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1322179.6250 - mean_squared_error: 1322179.6250\n",
      "Epoch 143/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1390316.3750 - mean_squared_error: 1390316.3750\n",
      "Epoch 144/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1339176.0000 - mean_squared_error: 1339176.0000\n",
      "Epoch 145/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1386128.1250 - mean_squared_error: 1386128.1250\n",
      "Epoch 146/3000\n",
      "96/96 [==============================] - 0s 843us/step - loss: 1387044.5000 - mean_squared_error: 1387044.5000\n",
      "Epoch 147/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1311910.8750 - mean_squared_error: 1311910.8750\n",
      "Epoch 148/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1335416.3750 - mean_squared_error: 1335416.3750\n",
      "Epoch 149/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1318436.8750 - mean_squared_error: 1318436.8750\n",
      "Epoch 150/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1380995.6250 - mean_squared_error: 1380995.6250\n",
      "Epoch 151/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1317095.1250 - mean_squared_error: 1317095.1250\n",
      "Epoch 152/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 1302559.2500 - mean_squared_error: 1302559.2500\n",
      "Epoch 153/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1346129.5000 - mean_squared_error: 1346129.5000\n",
      "Epoch 154/3000\n",
      "96/96 [==============================] - 0s 850us/step - loss: 1346811.1250 - mean_squared_error: 1346811.1250\n",
      "Epoch 155/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 1268556.0000 - mean_squared_error: 1268556.0000\n",
      "Epoch 156/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 1352114.8750 - mean_squared_error: 1352114.8750\n",
      "Epoch 157/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 1273376.6250 - mean_squared_error: 1273376.6250\n",
      "Epoch 158/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1349082.8750 - mean_squared_error: 1349082.8750\n",
      "Epoch 159/3000\n",
      "96/96 [==============================] - 0s 835us/step - loss: 1259646.3750 - mean_squared_error: 1259646.3750\n",
      "Epoch 160/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1300603.3750 - mean_squared_error: 1300603.3750\n",
      "Epoch 161/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1358095.8750 - mean_squared_error: 1358095.8750\n",
      "Epoch 162/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 1264192.8750 - mean_squared_error: 1264192.8750\n",
      "Epoch 163/3000\n",
      "96/96 [==============================] - 0s 859us/step - loss: 1229412.6250 - mean_squared_error: 1229412.6250\n",
      "Epoch 164/3000\n",
      "96/96 [==============================] - 0s 842us/step - loss: 1239648.3750 - mean_squared_error: 1239648.5000\n",
      "Epoch 165/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 1337598.8750 - mean_squared_error: 1337598.8750\n",
      "Epoch 166/3000\n",
      "96/96 [==============================] - 0s 887us/step - loss: 1252964.8750 - mean_squared_error: 1252964.8750\n",
      "Epoch 167/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 1178399.8750 - mean_squared_error: 1178399.8750\n",
      "Epoch 168/3000\n",
      "96/96 [==============================] - 0s 865us/step - loss: 1228132.7500 - mean_squared_error: 1228132.7500\n",
      "Epoch 169/3000\n",
      "96/96 [==============================] - 0s 929us/step - loss: 1151353.8750 - mean_squared_error: 1151353.8750\n",
      "Epoch 170/3000\n",
      "96/96 [==============================] - 0s 886us/step - loss: 1192020.2500 - mean_squared_error: 1192020.2500\n",
      "Epoch 171/3000\n",
      "96/96 [==============================] - 0s 867us/step - loss: 1244720.7500 - mean_squared_error: 1244720.6250\n",
      "Epoch 172/3000\n",
      "96/96 [==============================] - 0s 864us/step - loss: 1181804.7500 - mean_squared_error: 1181804.7500\n",
      "Epoch 173/3000\n",
      "96/96 [==============================] - 0s 860us/step - loss: 1161709.5000 - mean_squared_error: 1161709.5000\n",
      "Epoch 174/3000\n",
      "96/96 [==============================] - 0s 842us/step - loss: 1156062.6250 - mean_squared_error: 1156062.6250\n",
      "Epoch 175/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 1229544.0000 - mean_squared_error: 1229544.0000\n",
      "Epoch 176/3000\n",
      "96/96 [==============================] - 0s 849us/step - loss: 1155666.8750 - mean_squared_error: 1155666.8750\n",
      "Epoch 177/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1204979.8750 - mean_squared_error: 1204979.8750\n",
      "Epoch 178/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1182825.8750 - mean_squared_error: 1182825.8750\n",
      "Epoch 179/3000\n",
      "96/96 [==============================] - 0s 838us/step - loss: 1176569.3750 - mean_squared_error: 1176569.5000\n",
      "Epoch 180/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 1135848.2500 - mean_squared_error: 1135848.2500\n",
      "Epoch 181/3000\n",
      "96/96 [==============================] - 0s 849us/step - loss: 1119258.2500 - mean_squared_error: 1119258.2500\n",
      "Epoch 182/3000\n",
      "96/96 [==============================] - 0s 858us/step - loss: 1182248.5000 - mean_squared_error: 1182248.5000\n",
      "Epoch 183/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 1094358.1250 - mean_squared_error: 1094358.1250\n",
      "Epoch 184/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 1116943.7500 - mean_squared_error: 1116943.7500\n",
      "Epoch 185/3000\n",
      "96/96 [==============================] - 0s 860us/step - loss: 1081575.8750 - mean_squared_error: 1081575.8750\n",
      "Epoch 186/3000\n",
      "96/96 [==============================] - 0s 873us/step - loss: 1152521.8750 - mean_squared_error: 1152521.8750\n",
      "Epoch 187/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 1133573.5000 - mean_squared_error: 1133573.5000\n",
      "Epoch 188/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 1104910.3750 - mean_squared_error: 1104910.3750\n",
      "Epoch 189/3000\n",
      "96/96 [==============================] - 0s 873us/step - loss: 1127627.1250 - mean_squared_error: 1127627.1250\n",
      "Epoch 190/3000\n",
      "96/96 [==============================] - 0s 859us/step - loss: 1065624.1250 - mean_squared_error: 1065624.1250\n",
      "Epoch 191/3000\n",
      "96/96 [==============================] - 0s 849us/step - loss: 1140262.0000 - mean_squared_error: 1140262.0000\n",
      "Epoch 192/3000\n",
      "96/96 [==============================] - 0s 862us/step - loss: 1059240.6250 - mean_squared_error: 1059240.6250\n",
      "Epoch 193/3000\n",
      "96/96 [==============================] - 0s 938us/step - loss: 1066310.6250 - mean_squared_error: 1066310.6250\n",
      "Epoch 194/3000\n",
      "96/96 [==============================] - 0s 914us/step - loss: 1050669.2500 - mean_squared_error: 1050669.2500\n",
      "Epoch 195/3000\n",
      "96/96 [==============================] - 0s 892us/step - loss: 1036635.8125 - mean_squared_error: 1036635.8125\n",
      "Epoch 196/3000\n",
      "96/96 [==============================] - 0s 886us/step - loss: 1006048.8125 - mean_squared_error: 1006048.8125\n",
      "Epoch 197/3000\n",
      "96/96 [==============================] - 0s 905us/step - loss: 1033806.5625 - mean_squared_error: 1033806.5625\n",
      "Epoch 198/3000\n",
      "96/96 [==============================] - 0s 897us/step - loss: 1048564.6875 - mean_squared_error: 1048564.6875\n",
      "Epoch 199/3000\n",
      "96/96 [==============================] - 0s 921us/step - loss: 997506.6250 - mean_squared_error: 997506.6250\n",
      "Epoch 200/3000\n",
      "96/96 [==============================] - 0s 879us/step - loss: 1068991.6250 - mean_squared_error: 1068991.6250\n",
      "Epoch 201/3000\n",
      "96/96 [==============================] - 0s 898us/step - loss: 1027882.0000 - mean_squared_error: 1027882.0000\n",
      "Epoch 202/3000\n",
      "96/96 [==============================] - 0s 907us/step - loss: 974487.8125 - mean_squared_error: 974487.8125\n",
      "Epoch 203/3000\n",
      "96/96 [==============================] - 0s 921us/step - loss: 1047676.4375 - mean_squared_error: 1047676.4375\n",
      "Epoch 204/3000\n",
      "96/96 [==============================] - 0s 876us/step - loss: 1038167.6250 - mean_squared_error: 1038167.6250\n",
      "Epoch 205/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 1011235.3125 - mean_squared_error: 1011235.3125\n",
      "Epoch 206/3000\n",
      "96/96 [==============================] - 0s 851us/step - loss: 961819.3750 - mean_squared_error: 961819.3750\n",
      "Epoch 207/3000\n",
      "96/96 [==============================] - 0s 847us/step - loss: 1013985.3750 - mean_squared_error: 1013985.3750\n",
      "Epoch 208/3000\n",
      "96/96 [==============================] - 0s 862us/step - loss: 1014311.0000 - mean_squared_error: 1014311.0000\n",
      "Epoch 209/3000\n",
      "96/96 [==============================] - 0s 869us/step - loss: 996016.8125 - mean_squared_error: 996016.8125\n",
      "Epoch 210/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 942920.0000 - mean_squared_error: 942920.0000\n",
      "Epoch 211/3000\n",
      "96/96 [==============================] - 0s 909us/step - loss: 1004757.3750 - mean_squared_error: 1004757.3750\n",
      "Epoch 212/3000\n",
      "96/96 [==============================] - 0s 874us/step - loss: 945645.5625 - mean_squared_error: 945645.5625\n",
      "Epoch 213/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 943733.2500 - mean_squared_error: 943733.2500\n",
      "Epoch 214/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 870898.7500 - mean_squared_error: 870898.7500\n",
      "Epoch 215/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 965002.4375 - mean_squared_error: 965002.3750\n",
      "Epoch 216/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 1057033.7500 - mean_squared_error: 1057033.7500\n",
      "Epoch 217/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 966926.5625 - mean_squared_error: 966926.6875\n",
      "Epoch 218/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 964440.5625 - mean_squared_error: 964440.5625\n",
      "Epoch 219/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 927467.2500 - mean_squared_error: 927467.2500\n",
      "Epoch 220/3000\n",
      "96/96 [==============================] - 0s 843us/step - loss: 964585.0000 - mean_squared_error: 964585.0000\n",
      "Epoch 221/3000\n",
      "96/96 [==============================] - 0s 847us/step - loss: 908869.0625 - mean_squared_error: 908869.0625\n",
      "Epoch 222/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 934819.3750 - mean_squared_error: 934819.3750\n",
      "Epoch 223/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 962346.1875 - mean_squared_error: 962346.1875\n",
      "Epoch 224/3000\n",
      "96/96 [==============================] - 0s 852us/step - loss: 940320.1250 - mean_squared_error: 940320.1250\n",
      "Epoch 225/3000\n",
      "96/96 [==============================] - 0s 875us/step - loss: 858904.2500 - mean_squared_error: 858904.2500\n",
      "Epoch 226/3000\n",
      "96/96 [==============================] - 0s 843us/step - loss: 916793.8750 - mean_squared_error: 916793.8750\n",
      "Epoch 227/3000\n",
      "96/96 [==============================] - 0s 840us/step - loss: 853929.8750 - mean_squared_error: 853929.8750\n",
      "Epoch 228/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 855994.6250 - mean_squared_error: 855994.6250\n",
      "Epoch 229/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 891016.3125 - mean_squared_error: 891016.3125\n",
      "Epoch 230/3000\n",
      "96/96 [==============================] - 0s 839us/step - loss: 915602.8750 - mean_squared_error: 915602.9375\n",
      "Epoch 231/3000\n",
      "96/96 [==============================] - 0s 843us/step - loss: 857270.0625 - mean_squared_error: 857270.0625\n",
      "Epoch 232/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 878818.9375 - mean_squared_error: 878818.8750\n",
      "Epoch 233/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 870899.0000 - mean_squared_error: 870899.0000\n",
      "Epoch 234/3000\n",
      "96/96 [==============================] - 0s 858us/step - loss: 875986.1875 - mean_squared_error: 875986.1875\n",
      "Epoch 235/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 843078.4375 - mean_squared_error: 843078.4375\n",
      "Epoch 236/3000\n",
      "96/96 [==============================] - 0s 866us/step - loss: 814604.0000 - mean_squared_error: 814604.0000\n",
      "Epoch 237/3000\n",
      "96/96 [==============================] - 0s 867us/step - loss: 800134.6875 - mean_squared_error: 800134.6875\n",
      "Epoch 238/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 803647.0000 - mean_squared_error: 803646.8750\n",
      "Epoch 239/3000\n",
      "96/96 [==============================] - 0s 921us/step - loss: 862329.5625 - mean_squared_error: 862329.5625\n",
      "Epoch 240/3000\n",
      "96/96 [==============================] - 0s 933us/step - loss: 811129.4375 - mean_squared_error: 811129.4375\n",
      "Epoch 241/3000\n",
      "96/96 [==============================] - 0s 916us/step - loss: 816646.4375 - mean_squared_error: 816646.4375\n",
      "Epoch 242/3000\n",
      "96/96 [==============================] - 0s 903us/step - loss: 813887.6875 - mean_squared_error: 813887.6875\n",
      "Epoch 243/3000\n",
      "96/96 [==============================] - 0s 923us/step - loss: 749717.3125 - mean_squared_error: 749717.3125\n",
      "Epoch 244/3000\n",
      "96/96 [==============================] - 0s 908us/step - loss: 743938.8750 - mean_squared_error: 743938.8750\n",
      "Epoch 245/3000\n",
      "96/96 [==============================] - 0s 905us/step - loss: 840158.6250 - mean_squared_error: 840158.6250\n",
      "Epoch 246/3000\n",
      "96/96 [==============================] - 0s 969us/step - loss: 718829.8125 - mean_squared_error: 718829.8125\n",
      "Epoch 247/3000\n",
      "96/96 [==============================] - 0s 894us/step - loss: 816571.3125 - mean_squared_error: 816571.3125\n",
      "Epoch 248/3000\n",
      "96/96 [==============================] - 0s 888us/step - loss: 742909.6250 - mean_squared_error: 742909.6250\n",
      "Epoch 249/3000\n",
      "96/96 [==============================] - 0s 893us/step - loss: 747266.0000 - mean_squared_error: 747266.0000\n",
      "Epoch 250/3000\n",
      "96/96 [==============================] - 0s 889us/step - loss: 776363.0000 - mean_squared_error: 776363.0000\n",
      "Epoch 251/3000\n",
      "96/96 [==============================] - 0s 891us/step - loss: 717826.1250 - mean_squared_error: 717826.1250\n",
      "Epoch 252/3000\n",
      "96/96 [==============================] - 0s 914us/step - loss: 687914.3750 - mean_squared_error: 687914.3750\n",
      "Epoch 253/3000\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 726416.6250 - mean_squared_error: 726416.6250\n",
      "Epoch 254/3000\n",
      "96/96 [==============================] - 0s 905us/step - loss: 698646.0000 - mean_squared_error: 698646.0000\n",
      "Epoch 255/3000\n",
      "96/96 [==============================] - 0s 897us/step - loss: 691617.7500 - mean_squared_error: 691617.7500\n",
      "Epoch 256/3000\n",
      "96/96 [==============================] - 0s 897us/step - loss: 677864.0000 - mean_squared_error: 677864.0000\n",
      "Epoch 257/3000\n",
      "96/96 [==============================] - 0s 881us/step - loss: 766461.3750 - mean_squared_error: 766461.3750\n",
      "Epoch 258/3000\n",
      "96/96 [==============================] - 0s 907us/step - loss: 695440.3750 - mean_squared_error: 695440.3750\n",
      "Epoch 259/3000\n",
      "96/96 [==============================] - 0s 895us/step - loss: 707297.2500 - mean_squared_error: 707297.2500\n",
      "Epoch 260/3000\n",
      "96/96 [==============================] - 0s 940us/step - loss: 627209.3750 - mean_squared_error: 627209.3750\n",
      "Epoch 261/3000\n",
      "96/96 [==============================] - 0s 894us/step - loss: 699818.2500 - mean_squared_error: 699818.2500\n",
      "Epoch 262/3000\n",
      "96/96 [==============================] - 0s 909us/step - loss: 676305.6250 - mean_squared_error: 676305.6250\n",
      "Epoch 263/3000\n",
      "96/96 [==============================] - 0s 926us/step - loss: 700510.3750 - mean_squared_error: 700510.3750\n",
      "Epoch 264/3000\n",
      "96/96 [==============================] - 0s 911us/step - loss: 731087.8125 - mean_squared_error: 731087.8125\n",
      "Epoch 265/3000\n",
      "96/96 [==============================] - 0s 902us/step - loss: 860409.5625 - mean_squared_error: 860409.6250\n",
      "Epoch 266/3000\n",
      "96/96 [==============================] - 0s 909us/step - loss: 728176.1250 - mean_squared_error: 728176.1250\n",
      "Epoch 267/3000\n",
      "96/96 [==============================] - 0s 912us/step - loss: 727785.5625 - mean_squared_error: 727785.5625\n",
      "Epoch 268/3000\n",
      "96/96 [==============================] - 0s 918us/step - loss: 751620.7500 - mean_squared_error: 751620.7500\n",
      "Epoch 269/3000\n",
      "96/96 [==============================] - 0s 935us/step - loss: 655644.7500 - mean_squared_error: 655644.7500\n",
      "Epoch 270/3000\n",
      "96/96 [==============================] - 0s 928us/step - loss: 644232.2500 - mean_squared_error: 644232.2500\n",
      "Epoch 271/3000\n",
      "96/96 [==============================] - 0s 853us/step - loss: 672967.3750 - mean_squared_error: 672967.3750\n",
      "Epoch 272/3000\n",
      "96/96 [==============================] - 0s 907us/step - loss: 669248.1875 - mean_squared_error: 669248.1250\n",
      "Epoch 273/3000\n",
      "96/96 [==============================] - 0s 865us/step - loss: 707413.1250 - mean_squared_error: 707413.1250\n",
      "Epoch 274/3000\n",
      "96/96 [==============================] - 0s 884us/step - loss: 737139.0625 - mean_squared_error: 737139.0625\n",
      "Epoch 275/3000\n",
      "96/96 [==============================] - 0s 848us/step - loss: 685708.1875 - mean_squared_error: 685708.1875\n",
      "Epoch 276/3000\n",
      "96/96 [==============================] - 0s 844us/step - loss: 679228.9375 - mean_squared_error: 679228.9375\n",
      "Epoch 277/3000\n",
      "96/96 [==============================] - 0s 845us/step - loss: 643481.1250 - mean_squared_error: 643481.1250\n",
      "Epoch 278/3000\n",
      "96/96 [==============================] - 0s 882us/step - loss: 630520.9375 - mean_squared_error: 630520.9375\n",
      "Epoch 279/3000\n",
      "96/96 [==============================] - 0s 862us/step - loss: 581887.6250 - mean_squared_error: 581887.6875\n",
      "Epoch 280/3000\n",
      "96/96 [==============================] - 0s 888us/step - loss: 773809.6250 - mean_squared_error: 773809.6250\n",
      "Epoch 281/3000\n",
      "96/96 [==============================] - 0s 924us/step - loss: 735331.3125 - mean_squared_error: 735331.3125\n",
      "Epoch 282/3000\n",
      "96/96 [==============================] - 0s 872us/step - loss: 643607.8125 - mean_squared_error: 643607.8125\n",
      "Epoch 283/3000\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 563870.8125 - mean_squared_error: 563870.8125\n",
      "Epoch 284/3000\n",
      "96/96 [==============================] - 0s 972us/step - loss: 593176.1250 - mean_squared_error: 593176.1250\n",
      "Epoch 285/3000\n",
      "96/96 [==============================] - 0s 945us/step - loss: 605595.6875 - mean_squared_error: 605595.6875\n",
      "Epoch 286/3000\n",
      "96/96 [==============================] - 0s 909us/step - loss: 603601.7500 - mean_squared_error: 603601.7500\n",
      "Epoch 287/3000\n",
      "96/96 [==============================] - 0s 905us/step - loss: 599920.0625 - mean_squared_error: 599920.0625\n",
      "Epoch 288/3000\n",
      "96/96 [==============================] - 0s 929us/step - loss: 559932.8125 - mean_squared_error: 559932.8125\n",
      "Epoch 289/3000\n",
      "96/96 [==============================] - 0s 912us/step - loss: 593961.1875 - mean_squared_error: 593961.1875\n",
      "Epoch 290/3000\n",
      "96/96 [==============================] - 0s 963us/step - loss: 599163.6250 - mean_squared_error: 599163.6250\n",
      "Epoch 291/3000\n",
      "96/96 [==============================] - 0s 940us/step - loss: 571570.8750 - mean_squared_error: 571570.8750\n",
      "Epoch 292/3000\n",
      "96/96 [==============================] - 0s 912us/step - loss: 583386.5625 - mean_squared_error: 583386.5625\n",
      "Epoch 293/3000\n",
      "96/96 [==============================] - 0s 899us/step - loss: 612639.0000 - mean_squared_error: 612638.9375\n",
      "Epoch 294/3000\n",
      "96/96 [==============================] - 0s 989us/step - loss: 669080.5625 - mean_squared_error: 669080.5625\n",
      "Epoch 295/3000\n",
      "96/96 [==============================] - 0s 847us/step - loss: 800835.6875 - mean_squared_error: 800835.6875\n",
      "Epoch 296/3000\n",
      "96/96 [==============================] - 0s 882us/step - loss: 616297.5625 - mean_squared_error: 616297.5625\n",
      "Epoch 297/3000\n",
      "96/96 [==============================] - 0s 881us/step - loss: 549298.2500 - mean_squared_error: 549298.2500\n",
      "Epoch 298/3000\n",
      "96/96 [==============================] - 0s 896us/step - loss: 560681.3125 - mean_squared_error: 560681.3125\n",
      "Epoch 299/3000\n",
      "96/96 [==============================] - 0s 879us/step - loss: 543427.8125 - mean_squared_error: 543427.8125\n",
      "Epoch 300/3000\n",
      "96/96 [==============================] - 0s 889us/step - loss: 555323.3750 - mean_squared_error: 555323.3750\n",
      "Epoch 301/3000\n",
      "96/96 [==============================] - 0s 856us/step - loss: 529571.6875 - mean_squared_error: 529571.6875\n",
      "Epoch 302/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 561962.1875 - mean_squared_error: 561962.1875\n",
      "Epoch 303/3000\n",
      "96/96 [==============================] - 0s 872us/step - loss: 532816.0625 - mean_squared_error: 532816.0625\n",
      "Epoch 304/3000\n",
      "96/96 [==============================] - 0s 867us/step - loss: 553982.4375 - mean_squared_error: 553982.4375\n",
      "Epoch 305/3000\n",
      "96/96 [==============================] - 0s 865us/step - loss: 587935.9375 - mean_squared_error: 587935.9375\n",
      "Epoch 306/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 544389.9375 - mean_squared_error: 544389.9375\n",
      "Epoch 307/3000\n",
      "96/96 [==============================] - 0s 861us/step - loss: 534049.7500 - mean_squared_error: 534049.7500\n",
      "Epoch 308/3000\n",
      "96/96 [==============================] - 0s 857us/step - loss: 593544.6250 - mean_squared_error: 593544.6250\n",
      "Epoch 309/3000\n",
      "96/96 [==============================] - 0s 846us/step - loss: 530858.2500 - mean_squared_error: 530858.2500\n",
      "Epoch 310/3000\n",
      "96/96 [==============================] - 0s 864us/step - loss: 590273.1875 - mean_squared_error: 590273.1875\n",
      "Epoch 311/3000\n",
      "96/96 [==============================] - 0s 858us/step - loss: 559537.1250 - mean_squared_error: 559537.1250\n",
      "Epoch 312/3000\n",
      "96/96 [==============================] - 0s 841us/step - loss: 527979.2500 - mean_squared_error: 527979.2500\n",
      "Epoch 313/3000\n",
      "96/96 [==============================] - 0s 837us/step - loss: 479266.0000 - mean_squared_error: 479266.0000\n",
      "Epoch 314/3000\n",
      "96/96 [==============================] - 0s 868us/step - loss: 529136.1250 - mean_squared_error: 529136.1250\n",
      "Epoch 315/3000\n",
      "96/96 [==============================] - 0s 878us/step - loss: 514616.3438 - mean_squared_error: 514616.3438\n",
      "Epoch 316/3000\n",
      "59/96 [=================>............] - ETA: 0s - loss: 465806.0625 - mean_squared_error: 465806.0625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb#ch0000005?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMeanSquaredError\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mopt, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mMeanSquaredError\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb#ch0000005?line=22'>23</a>\u001b[0m \u001b[39m# fit the keras model on the dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb#ch0000005?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb#ch0000005?line=24'>25</a>\u001b[0m eva \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test,y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jason/Desktop/STUDY/Coursework/760Project/project_part1_death.ipynb#ch0000005?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfold \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(counter)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m eva: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(eva[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:474\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=453'>454</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, ctx, args, cancellation_manager\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=454'>455</a>\u001b[0m   \u001b[39m\"\"\"Calls this function with `args` as inputs.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=455'>456</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=456'>457</a>\u001b[0m \u001b[39m  `ConcreteFunction` execution respects device annotations only if the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=471'>472</a>\u001b[0m \u001b[39m      available to be called because it has been garbage collected.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=472'>473</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=473'>474</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49minput_arg):\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=474'>475</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=475'>476</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSignature specifies \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39minput_arg))\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=476'>477</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marguments, got: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=478'>479</a>\u001b[0m   \u001b[39m# If the `ScopedTFFunction` (accessed via `_c_func`) has already been\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=479'>480</a>\u001b[0m   \u001b[39m# cleaned up as a part of garbage collection, this `_EagerDefinedFunction`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=480'>481</a>\u001b[0m   \u001b[39m# should also be garbage and is likely being called as part of a `__del__`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=481'>482</a>\u001b[0m   \u001b[39m# elsewhere. In that case, there's nothing we can do, so we raise an\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=482'>483</a>\u001b[0m   \u001b[39m# exception for the caller to handle.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model use the 549 samples, with clinic data + ct data\n",
    "if re_train_1 == True:\n",
    "    models = []\n",
    "    evas = []\n",
    "    counter = 0\n",
    "    for train_index,test_index in KFold(n_split).split(X_fill_train):\n",
    "        print(\"training fold \"+str(counter)+\" total \"+str(n_split) +\" folds\")\n",
    "\n",
    "        x_train,x_test = X_fill_train[train_index], X_fill_train[test_index]\n",
    "        y_train,y_test = y_fill_train[train_index], y_fill_train[test_index]\n",
    "    \n",
    "        # define the keras model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=21, use_bias=True))\n",
    "        model.add(Dense(64, activation='relu',use_bias=True))\n",
    "        model.add(Dense(128, activation='relu',use_bias=True))\n",
    "        model.add(Dense(256, activation='relu',use_bias=True))\n",
    "        model.add(Dense(512, activation='relu',use_bias=True))\n",
    "        model.add(Dense(1))\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        # compile the keras model\n",
    "        model.compile(loss='MeanSquaredError', optimizer=opt, metrics=['MeanSquaredError'])\n",
    "        # fit the keras model on the dataset\n",
    "        model.fit(x_train, y_train, epochs=3000, batch_size=5,verbose=1)\n",
    "        eva = model.evaluate(x_test,y_test)\n",
    "        \n",
    "        print(\"fold \"+str(counter)+\" eva: \"+str(eva[0]))\n",
    "        counter = counter + 1\n",
    "        \n",
    "        models.append(model)\n",
    "        evas.append(eva)\n",
    "    for i in range(len(evas)):\n",
    "        evas[i] = evas[i][0]\n",
    "    # find the best evaluated models\n",
    "    model = models[evas.index(min(evas))]\n",
    "\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"NN_models/keras_model_step_1\", custom_objects=None, compile=True, options=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average death day error across all 549 traing sample: 420.6206548027219\n"
     ]
    }
   ],
   "source": [
    "# let's see how the model fits the original training data\n",
    "y_fill_train_predict = model.predict(X_fill_train)\n",
    "err_sum = 0\n",
    "for i in range((y_fill_train_predict.shape)[0]):\n",
    "    err_sum = err_sum + (abs(y_fill_train_predict[i,0]-y_fill_train[i,0]))\n",
    "print(\"average death day error across all 549 traing sample: \"+str(err_sum/(y_fill_train_predict.shape)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 17:51:28.585930: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: NN_models/keras_model_step_1/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model used for step 1\n",
    "from datetime import datetime\n",
    "if re_train_1 == True:\n",
    "    model.save(\"NN_models/keras_model_step_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9223, 25)\n",
      "(9223, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_368642/407643235.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np_outcome = np.array(Outcome_Data)\n"
     ]
    }
   ],
   "source": [
    "# fill in the missing death day data using both clinic data and ct data\n",
    "y_fill_test_predict = model.predict(X_fill_test)\n",
    "for i in range(len(fill_test_idx)):\n",
    "    if y_fill_test_predict[i] < 0:\n",
    "        Outcome_Data[0][fill_test_idx[i]] = 0\n",
    "    else:\n",
    "        Outcome_Data[0][fill_test_idx[i]] = y_fill_test_predict[i]\n",
    "\n",
    "np_outcome = np.array(Outcome_Data)\n",
    "np_ctdata = np.array(CT_Data)\n",
    "np_outcome = np_outcome.T\n",
    "np_ctdata = np_ctdata.T\n",
    "print(np_outcome.shape)\n",
    "print(np_ctdata.shape)\n",
    "\n",
    "out_csv = np.column_stack((np_outcome,np_ctdata))\n",
    "out_csv = out_csv.astype(float)\n",
    "\n",
    "np.savetxt(\"test_death_day_1.csv\", out_csv, fmt='%f',delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Use only ct data to predict death day\n",
    "Note that we have only around 549 real samples with known death day for us to train the model\n",
    "\n",
    "In this case, we use only CT data to predict death, we use the 549 samples with death day as training set, and predict the samples with missing death day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reread the data to clear variables\n",
    "Clininc_Data,Outcome_Data,CT_Data = read_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9223, 11)\n",
      "(549, 11)\n",
      "(8674, 11)\n",
      "(549, 1)\n"
     ]
    }
   ],
   "source": [
    "# prepare data for training, this case we use ct data only\n",
    "X1 = np.array(CT_Data)\n",
    "X1 = X1.T\n",
    "X1 = X1.astype(float)\n",
    "print(X1.shape)\n",
    "\n",
    "X_fill_train_2 = X1[fill_train_idx,:]\n",
    "print(X_fill_train_2.shape)\n",
    "\n",
    "X_fill_test_2 = X1[fill_test_idx,:]\n",
    "print(X_fill_test_2.shape)\n",
    "\n",
    "y_fill_train_2 = np.array(Outcome_Data[0])\n",
    "y_fill_train_2 = y_fill_train_2.astype(int)\n",
    "y_fill_train_2 = np.asmatrix(y_fill_train_2)\n",
    "y_fill_train_2 = y_fill_train_2.T\n",
    "y_fill_train_2 = y_fill_train_2[fill_train_idx,:]\n",
    "print(y_fill_train_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model used for step 2\n",
    "if re_train_2 == True:\n",
    "    models_2 = []\n",
    "    evas_2 = []\n",
    "    counter = 0\n",
    "    for train_index,test_index in KFold(n_split).split(X_fill_train_2):\n",
    "        print(\"training fold \"+str(counter)+\" total \"+str(n_split) +\" folds\")\n",
    "\n",
    "        x_train,x_test = X_fill_train_2[train_index], X_fill_train_2[test_index]\n",
    "        y_train,y_test = y_fill_train_2[train_index], y_fill_train_2[test_index]\n",
    "        # define the keras model\n",
    "        model_2 = Sequential()\n",
    "        model_2.add(Dense(32, input_dim=11, use_bias=True))\n",
    "        model_2.add(Dense(64, activation='relu',use_bias=True))\n",
    "        model_2.add(Dense(128, activation='relu',use_bias=True))\n",
    "        model_2.add(Dense(256, activation='relu',use_bias=True))\n",
    "        model_2.add(Dense(512, activation='relu',use_bias=True))\n",
    "        model_2.add(Dense(1))\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0004)\n",
    "        # compile the keras model\n",
    "        model_2.compile(loss='MeanSquaredError', optimizer=opt, metrics=['MeanSquaredError'])\n",
    "        # fit the keras model on the dataset\n",
    "        model_2.fit(x_train, y_train, epochs=5000, batch_size=10,verbose=1)\n",
    "        eva_2 = model_2.evaluate(x_test,y_test)\n",
    "\n",
    "        print(\"fold \"+str(counter)+\" eva: \"+str(eva_2[0]))\n",
    "        counter = counter + 1\n",
    "\n",
    "        models_2.append(model_2)\n",
    "        evas_2.append(eva_2)\n",
    "    for i in range(len(evas_2)):\n",
    "        evas_2[i] = evas_2[i][0]\n",
    "    # find the best evaluated models\n",
    "    model_2 = models_2[evas_2.index(min(evas_2))]\n",
    "else:\n",
    "    model_2 = tf.keras.models.load_model(\"NN_models/keras_model_step_2\", custom_objects=None, compile=True, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 602us/step - loss: 3.3247 - mean_squared_error: 3.3247\n",
      "[3.324714183807373, 3.324714183807373]\n",
      "average death day error across all 549 traing sample: 1.1271004911328926\n"
     ]
    }
   ],
   "source": [
    "# save/evaluate the model\n",
    "from datetime import datetime\n",
    "if re_train_2 == True:\n",
    "    model_2.save(\"NN_models/keras_model_step_2\")\n",
    "score_2 = model_2.evaluate(X_fill_train_2, y_fill_train_2, verbose = 1) \n",
    "print(score_2)\n",
    "\n",
    "# let's see how the model fits the original training set\n",
    "err_sum = 0\n",
    "y_fill_train_predict_2 = model_2.predict(X_fill_train_2)\n",
    "for i in range((y_fill_train_predict_2.shape)[0]):\n",
    "    err_sum = err_sum + (abs(y_fill_train_predict_2[i,0]-y_fill_train_2[i,0]))\n",
    "print(\"average death day error across all 549 traing sample: \"+str(err_sum/(y_fill_train_predict_2.shape)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9223, 25)\n",
      "(9223, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_368642/3638360185.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np_outcome = np.array(Outcome_Data)\n"
     ]
    }
   ],
   "source": [
    "# predict and fill the missing death day field\n",
    "y_fill_test_predict_2 = model_2.predict(X_fill_test_2)\n",
    "\n",
    "for i in range(len(fill_test_idx)):\n",
    "    if y_fill_test_predict_2[i] < 0:\n",
    "        Outcome_Data[0][fill_test_idx[i]] = 0\n",
    "    else:\n",
    "        Outcome_Data[0][fill_test_idx[i]] = y_fill_test_predict_2[i]\n",
    "\n",
    "np_outcome = np.array(Outcome_Data)\n",
    "np_ctdata = np.array(CT_Data)\n",
    "np_outcome = np_outcome.T\n",
    "np_ctdata = np_ctdata.T\n",
    "print(np_outcome.shape)\n",
    "print(np_ctdata.shape)\n",
    "\n",
    "out_csv = np.column_stack((np_outcome,np_ctdata))\n",
    "out_csv = out_csv.astype(float)\n",
    "\n",
    "np.savetxt(\"test_death_day_2.csv\", out_csv, fmt='%f',delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
